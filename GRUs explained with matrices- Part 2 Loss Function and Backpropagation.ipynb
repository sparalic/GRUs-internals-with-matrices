{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "%autosave 180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap:\n",
    "In part one of this tutorial series, we demonstrated the matrix operations used to estimate the hidden states and outputs for the forward pass of a GRU. Based on the poor results we obvioulsy need to optimize our algorithm and test it on a test set to ensure generalizability. This is typically done using several steps/techniques. In this tutorial we will walkthrough what happens under the hood during optimization, specifically calculating the loss function and performing backpropagation through time to update the weights over several epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be our input ---> x\n",
    "text = 'MathMathMathMathMath'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer representation of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_list = list(set(text))   # get all of the unique letters in our text variable\n",
    "vocabulary_size = len(character_list)   # count the number of unique elements\n",
    "character_dictionary = {'h': 0, 'a': 1, 't': 2, 'M': 3}\n",
    "# {char:e for e, char in enumerate(character_list)}  # create a dictionary mapping each unique char to a number\n",
    "encoded_chars = [character_dictionary[char] for char in text] #integer representation of our vocabulary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(encoded, vocab_size):\n",
    "    result = torch.zeros((len(encoded), vocab_size))\n",
    "    for i, idx in enumerate(encoded):\n",
    "        result[i, idx] = 1.0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode our encoded charactes\n",
    "batch_size = 2\n",
    "seq_length = 3\n",
    "num_samples = (len(encoded_chars) - 1) // seq_length # time lag of 1 for creating the labels\n",
    "vocab_size = 4\n",
    "\n",
    "data = one_hot_encode(encoded_chars[:seq_length*num_samples], vocab_size).reshape((num_samples, seq_length, vocab_size))\n",
    "num_batches = len(data) // batch_size\n",
    "X = data[:num_batches*batch_size].reshape((num_batches, batch_size, seq_length, vocab_size))\n",
    "# swap batch_size and seq_length axis to make later access easier\n",
    "X = X.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# +1 shift the labels by one so that given the previous letter the char we should predict would be or next char\n",
    "labels = one_hot_encode(encoded_chars[1:seq_length*num_samples+1], vocab_size) \n",
    "y = labels.reshape((num_batches, batch_size, seq_length, vocab_size))\n",
    "y = y.transpose(1, 2) # transpose the first and second index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intitialize weight matrices and bias vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1) # reproducibility\n",
    "\n",
    "####  Define the network parameters:\n",
    "hiddenSize = 2 # network size, this can be any number (depending on your task)\n",
    "numClass = 4 # this is the same as our vocab_size\n",
    "\n",
    "#### Weight matrices for our inputs \n",
    "Wz = torch.randn(vocab_size, hiddenSize)\n",
    "Wr = torch.randn(vocab_size, hiddenSize)\n",
    "Wh = torch.randn(vocab_size, hiddenSize)\n",
    "\n",
    "## Intialize the hidden state\n",
    "# this is for demonstration purposes only, in the actual model it will be initiated during training a loop over the \n",
    "# the number of bacthes and updated before passing to the next GRU cell.\n",
    "h_t_demo = torch.zeros(batch_size, hiddenSize) \n",
    "\n",
    "#### Weight matrices for our hidden layer\n",
    "Uz = torch.randn(hiddenSize, hiddenSize)\n",
    "Ur = torch.randn(hiddenSize, hiddenSize)\n",
    "Uh = torch.randn(hiddenSize, hiddenSize)\n",
    "\n",
    "#### bias vectors for our hidden layer\n",
    "bz = torch.zeros(hiddenSize)\n",
    "br = torch.zeros(hiddenSize)\n",
    "bh = torch.zeros(hiddenSize)\n",
    "\n",
    "#### Output weights\n",
    "Wy = torch.randn(hiddenSize, numClass)\n",
    "by = torch.zeros(numClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(x, h):\n",
    "    outputs = []\n",
    "    for i in range(num_batches):  # this loops over the batches \n",
    "        x = X[i]\n",
    "        for i,sequence in enumerate(x): # iterates over the sequences in each batch\n",
    "            z = torch.sigmoid(torch.matmul(sequence, Wz) + torch.matmul(h, Uz) + bz)\n",
    "            r = torch.sigmoid(torch.matmul(sequence, Wr) + torch.matmul(h, Ur) + br)\n",
    "            h_tilde = torch.tanh(torch.matmul(sequence, Wh) + torch.matmul(r * h, Uh) + bh)\n",
    "            h = z * h + (1 - z) * h_tilde\n",
    "\n",
    "            # Linear layer\n",
    "            y_linear = torch.matmul(h, Wy) + by\n",
    "\n",
    "            # Softmax activation function\n",
    "            y_t = F.softmax(y_linear, dim=1)\n",
    "\n",
    "            outputs.append(y_t)\n",
    "        return torch.stack(outputs), h\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(primer, length_chars_predict):\n",
    "    \n",
    "    word = primer\n",
    "\n",
    "    primer_dictionary = [character_dictionary[char] for char in word]\n",
    "    test_input = one_hot_encode(primer_dictionary, vocab_size)\n",
    "    \n",
    "\n",
    "    h = torch.zeros(1, hiddenSize)\n",
    "\n",
    "    for i in range(length_chars_predict):\n",
    "        outputs, h = gru(test_input, h)\n",
    "        choice = np.random.choice(vocab_size, p=outputs[-1][0].numpy())\n",
    "        word += character_list[choice]\n",
    "        input_sequence = one_hot_encode([choice],vocab_size)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h0:tensor([[ 0.7565, -0.3472],\n",
      "        [-0.1355, -0.2040]])\n",
      "h1:tensor([[-0.1535, -0.5712],\n",
      "        [ 0.7664, -0.5062]])\n",
      "h2:tensor([[ 0.7495, -0.8616],\n",
      "        [-0.2399, -0.6680]])\n"
     ]
    }
   ],
   "source": [
    "# h gets updated and then we calculate for the next \n",
    "h_t_1 = []\n",
    "h = h_t_demo\n",
    "for i,sequence in enumerate(X[0]):   # iterate over each sequence in the batch to calculate the hidden state h \n",
    "    z = torch.sigmoid(torch.matmul(sequence, Wz) + torch.matmul(h, Uz) + bz)\n",
    "    r = torch.sigmoid(torch.matmul(sequence, Wr) + torch.matmul(h, Ur) + br)\n",
    "    h_tilde = torch.tanh(torch.matmul(sequence, Wh) + torch.matmul(r * h, Uh) + bh)\n",
    "    h = z * h + (1 - z) * h_tilde\n",
    "    h_t_1.append(h)\n",
    "    print(f'h{i}:{h}')\n",
    "h_t_1 = torch.stack(h_t_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "htensor([[ 0.7495, -0.8616],\n",
      "        [-0.2399, -0.6680]]) at i0\n",
      "MatMhMhhahhthhMMtMhhhM\n",
      "htensor([[ 0.7937, -0.8401],\n",
      "        [-0.2344, -0.6548]]) at i1\n",
      "MahMMaMtttththhtthaMaM\n",
      "htensor([[ 0.7943, -0.8410],\n",
      "        [-0.2364, -0.6575]]) at i2\n",
      "MaMMMahtthMahhMhthaMth\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 1  # passes through the data\n",
    "for e in range(max_epochs):\n",
    "    h = torch.zeros(batch_size, hiddenSize)\n",
    "    for i in range(num_batches):\n",
    "        x_in = X[i]\n",
    "        y_in = y[i]\n",
    "        \n",
    "        out, h = gru(x_in, h)\n",
    "        print(f'h{h} at i{i}')\n",
    "\n",
    "\n",
    "        print(sample('Ma',20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happen here?\n",
    "As we pointed out in the first tutorial the first couple of strings generated are a bit erratic, but after a few passes it seems to get at least the next two characters correct. However, in order to measure how inconsistent our predictions are versus the true labels, we need a metric. This metric is call the loss function, that measures how well the model is performing. It is a positive value that decreases as the network becomes more confident with it's predictions. This loss function for multiclass classification problems is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Cross Entropy = $-\\frac{1}{N}\\sum_{j}^M {y * log(\\hat{y}})$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{loss}(x, class) = -\\log\\left(\\frac{\\exp(x[class])}{\\sum_j \\exp(x[j])}\\right)\n",
    "                   = -x[class] + \\log\\left(\\sum_j \\exp(x[j])\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4662, 0.1189, 0.2620, 0.1529],\n",
       "         [0.1668, 0.2208, 0.4420, 0.1704]],\n",
       "\n",
       "        [[0.1353, 0.1514, 0.6092, 0.1041],\n",
       "         [0.4362, 0.1577, 0.1986, 0.2074]],\n",
       "\n",
       "        [[0.4401, 0.1330, 0.2603, 0.1666],\n",
       "         [0.1831, 0.1812, 0.4930, 0.1426]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7943, -0.8410],\n",
       "        [-0.2364, -0.6575]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
