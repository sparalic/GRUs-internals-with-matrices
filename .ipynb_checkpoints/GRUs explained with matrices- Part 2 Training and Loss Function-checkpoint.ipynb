{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRUs explained with matrices: Part 2 Training and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "%autosave 180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap:\n",
    "In part one of this tutorial series, we demonstrated the matrix operations used to estimate the hidden states and outputs for the forward pass of a GRU. Based on our poor results, we obvioulsy need to optimize our algorithm and test it on a test set to ensure generalizability. This is typically done using several steps/techniques. In this tutorial we will walkthrough what happens under the hood during optimization, specifically calculating the loss function and performing backpropagation through time to update the weights over several epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be our input ---> x\n",
    "text = 'MathMathMathMathMath'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer representation of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_list = list(set(text))   # get all of the unique letters in our text variable\n",
    "vocabulary_size = len(character_list)   # count the number of unique elements\n",
    "character_dictionary = {'h': 0, 'a': 1, 't': 2, 'M': 3}\n",
    "# {char:e for e, char in enumerate(character_list)}  # create a dictionary mapping each unique char to a number\n",
    "encoded_chars = [character_dictionary[char] for char in text] #integer representation of our vocabulary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(encoded,vocab_size):\n",
    "    result = torch.zeros((len(encoded), vocab_size))\n",
    "    for i, idx in enumerate(encoded):\n",
    "        result[i, idx] = 1.0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode our encoded charactes\n",
    "batch_size = 2\n",
    "seq_length = 3\n",
    "num_samples = (len(encoded_chars) - 1) // seq_length # time lag of 1 for creating the labels\n",
    "vocab_size = 4\n",
    "\n",
    "data = one_hot_encode(encoded_chars[:seq_length*num_samples], vocab_size).reshape((num_samples, seq_length, vocab_size))\n",
    "num_batches = len(data) // batch_size\n",
    "X = data[:num_batches*batch_size].reshape((num_batches, batch_size, seq_length, vocab_size))\n",
    "# swap batch_size and seq_length axis to make later access easier\n",
    "X = X.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# +1 shift the labels by one so that given the previous letter the char we should predict would be or next char\n",
    "labels = one_hot_encode(encoded_chars[1:seq_length*num_samples+1],vocab_size=vocab_size) \n",
    "y = labels.reshape((num_batches, batch_size, seq_length, vocab_size))\n",
    "y = y.transpose(1, 2) # transpose the first and second index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intitialize weight matrices and bias vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1) # reproducibility\n",
    "\n",
    "####  Define the network parameters:\n",
    "hiddenSize = 2 # network size, this can be any number (depending on your task)\n",
    "numClass = 4 # this is the same as our vocab_size\n",
    "\n",
    "#### Weight matrices for our inputs \n",
    "Wz = Variable(torch.randn(vocab_size, hiddenSize), requires_grad=True)\n",
    "Wr = Variable(torch.randn(vocab_size, hiddenSize), requires_grad=True)\n",
    "Wh = Variable(torch.randn(vocab_size, hiddenSize), requires_grad=True) \n",
    "\n",
    "#### Weight matrices for our hidden layer\n",
    "Uz = Variable(torch.randn(hiddenSize, hiddenSize), requires_grad=True)\n",
    "Ur = Variable(torch.randn(hiddenSize, hiddenSize), requires_grad=True)\n",
    "Uh = Variable(torch.randn(hiddenSize, hiddenSize), requires_grad=True)\n",
    "\n",
    "#### bias vectors for our hidden layer\n",
    "bz = Variable(torch.zeros(hiddenSize), requires_grad=True)\n",
    "br = Variable(torch.zeros(hiddenSize), requires_grad=True)\n",
    "bh = Variable(torch.zeros(hiddenSize), requires_grad=True)\n",
    "\n",
    "#### Output weights\n",
    "Wy = Variable(torch.randn(hiddenSize, numClass), requires_grad=True)\n",
    "by = Variable(torch.zeros(numClass), requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(x, h):\n",
    "    outputs = []\n",
    "    for sequence in x: # iterates over the sequences in each batch\n",
    "        z = torch.sigmoid(torch.matmul(sequence, Wz) + torch.matmul(h, Uz) + bz)\n",
    "        r = torch.sigmoid(torch.matmul(sequence, Wr) + torch.matmul(h, Ur) + br)\n",
    "        h_tilde = torch.tanh(torch.matmul(sequence, Wh) + torch.matmul(r * h, Uh) + bh)\n",
    "        h = z * h + (1 - z) * h_tilde\n",
    "\n",
    "        #Linear layer\n",
    "        y_linear = torch.matmul(h, Wy) + by\n",
    "        \n",
    "        # Softmax activation function\n",
    "        y_t = F.softmax(y_linear, dim=1)\n",
    "        outputs.append(y_t)\n",
    "    return torch.stack(outputs), h\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(primer, length_chars_predict):\n",
    "    \n",
    "    word = primer\n",
    "\n",
    "    primer_dictionary = [character_dictionary[char] for char in word]\n",
    "    test_input = one_hot_encode(primer_dictionary, vocab_size)\n",
    "    \n",
    "\n",
    "    h = torch.zeros(1, hiddenSize)\n",
    "\n",
    "    for i in range(length_chars_predict):\n",
    "        outputs, h = gru(test_input, h)\n",
    "        choice = np.random.choice(vocab_size, p=outputs[-1][0].detach().numpy())\n",
    "        word += character_list[choice]\n",
    "        input_sequence = one_hot_encode([choice],vocab_size)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MataMttataatthtthaMhMa\n",
      "MattatttttattatatataMt\n",
      "MattMtMttttthtMtatMtth\n",
      "MaMtttttttttatttttatht\n",
      "MattattMattttMtttthtMt\n",
      "MattataahMtataatttttMt\n",
      "MahMtMMtttMttMMaMMthaM\n",
      "MatttMttttttMttttattah\n",
      "MahMtMMhhatMaMattttaMt\n",
      "MaMMtMMhtttththhMtthth\n",
      "MaMthttMthhttaathtMMta\n",
      "MaMMaaaahtMhhttatthtMt\n",
      "MaataMtthMtMththtMMatt\n",
      "MahhhtthMMthtMtatMtttt\n",
      "MahtMhttttaattMttttMaa\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 5  # passes through the data\n",
    "for e in range(max_epochs):\n",
    "    h = torch.zeros(batch_size, hiddenSize)\n",
    "    for i in range(num_batches):\n",
    "        x_in = X[i]\n",
    "        y_in = y[i]\n",
    "\n",
    "        out, h = gru(x_in, h)\n",
    "        print(sample('Ma',20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's happening here?\n",
    "As we pointed out in the first tutorial the first couple of strings generated are a bit erratic, but after a few passes it seems to get at least the next two characters correct. However, in order to measure how inconsistent our predictions are versus the true labels, we need a metric. This metric is call the loss function, that measures how well the model is performing. It is a positive value that decreases as the network becomes more confident with it's predictions. This loss function for multiclass classification problems is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Cross Entropy = $-\\frac{1}{N}\\sum_{j} {y_j * log(\\hat{y_j}})$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, our calculated hidden states and predicted outputs for the first batch? This picture seems a bit busy, however the goal here is to visualize what you outputs and hidden states actually look like under the hood. The predictions are probabilities which were calculated using the Softmax activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/hidden_image.png\" style=\"height:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-run the training loop storing the outputs (  $\\hat{y}$) and hidden states ($h_{(t-1)}, h_t, and,  h_{(t+1)} )$ for each sequence in batch 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustration in code:\n",
    "To understand what is happening you will notice that we work from the inside out, before moving to functions. Here, we are grabbing the outputs and hidden states calculated with just two loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_2 = [] # stores the calculated h for each input x\n",
    "outputs = []\n",
    "h = torch.zeros(batch_size, hiddenSize) # intitalizes the hidden state\n",
    "for i in range(num_batches):  # this loops over the batches \n",
    "    x = X[i]\n",
    "    for i,sequence in enumerate(x): # iterates over the sequences in each batch\n",
    "        z = torch.sigmoid(torch.matmul(sequence, Wz) + torch.matmul(h, Uz) + bz)\n",
    "        r = torch.sigmoid(torch.matmul(sequence, Wr) + torch.matmul(h, Ur) + br)\n",
    "        h_tilde = torch.tanh(torch.matmul(sequence, Wh) + torch.matmul(r * h, Uh) + bh)\n",
    "        h = z * h + (1 - z) * h_tilde\n",
    "        \n",
    "        # Linear layer\n",
    "        y_linear = torch.matmul(h, Wy) + by\n",
    "        \n",
    "        # Softmax activation function\n",
    "        y_t = F.softmax(y_linear, dim=1)\n",
    "        \n",
    "        ht_2.append(h)\n",
    "        outputs.append(y_t)\n",
    "        \n",
    "ht_2 = torch.stack(ht_2)\n",
    "outputs = torch.stack(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross entropy loss is first calculated for each sequence in the batch then averaged over all sequences. So, in this example we will calculate the cross entropy loss for each sequence from scratch. But first, let's grab the predictions made on the first batch. To do this we will grab the for element ( index 0) from our ht_2 and ouputs variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Predictions for the first batch: \n",
      "\n",
      "tensor([[[0.4342, 0.1669, 0.1735, 0.2254],\n",
      "         [0.2207, 0.2352, 0.3322, 0.2119]],\n",
      "\n",
      "        [[0.2045, 0.1916, 0.4443, 0.1596],\n",
      "         [0.4384, 0.1563, 0.1995, 0.2058]],\n",
      "\n",
      "        [[0.4261, 0.1340, 0.2763, 0.1636],\n",
      "         [0.1819, 0.1798, 0.4972, 0.1411]]], grad_fn=<SliceBackward>),       \n",
      " \n",
      " Hidden states for the first bactch: \n",
      "tensor([[[ 0.7565, -0.3472],\n",
      "         [-0.1355, -0.2040]],\n",
      "\n",
      "        [[-0.1535, -0.5712],\n",
      "         [ 0.7664, -0.5062]],\n",
      "\n",
      "        [[ 0.7495, -0.8616],\n",
      "         [-0.2399, -0.6680]]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "hidden_batch_1 = ht_2[:3]\n",
    "outputs_batch_1 = outputs[:3]\n",
    "print(f' Predictions for the first batch: \\n\\n{outputs_batch_1}, \\\n",
    "      \\n \\n Hidden states for the first bactch: \\n{hidden_batch_1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How well did we perform?\n",
    "Let's look at our lables for batch 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the output probabilities we can tell that we did not do so well. However, let's quantify it using the cross entropy equation! Here we will work our way from the inner term out on the first sequence in the batch. Note, the code will included all 3 sequences in batch 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/cross_en.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First term:** Element-wise multiplication of the true labels with the log of the predicted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/cross_t1.png\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0000, -1.7905, -0.0000, -0.0000],\n",
       "         [-0.0000, -0.0000, -0.0000, -1.5516]],\n",
       "\n",
       "        [[-0.0000, -0.0000, -0.8114, -0.0000],\n",
       "         [-0.0000, -1.8560, -0.0000, -0.0000]],\n",
       "\n",
       "        [[-0.8532, -0.0000, -0.0000, -0.0000],\n",
       "         [-0.0000, -0.0000, -0.6987, -0.0000]]], grad_fn=<ThMulBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0] * torch.log(outputs_batch_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second term:** Summation of remaining values within each sequence. In this step, it is key to note that the axis will be reduced row-wise, only containing the non-zero terms. This will be done in a loop programatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/cross_t2.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7905, -1.5516],\n",
       "        [-0.8114, -1.8560],\n",
       "        [-0.8532, -0.6987]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce_sums = []\n",
    "for prediction, label in zip(outputs_batch_1, y[0]):\n",
    "    ce_sum = torch.sum(label * torch.log(prediction),dim=1)\n",
    "    ce_sums.append(ce_sum)\n",
    "ce_sums = torch.stack(ce_sums)\n",
    "ce_sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Third term:** Mean of the reduced samples for first sequence within the batch tow-wise. This example calculation was done on the first sequence within batch 1. However, the code implementation covers all 3 sequences in batch 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/cross_t3.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.6710, 1.3337, 0.7760], grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce_scores = []\n",
    "for ce in ce_sums:\n",
    "    ce = -torch.mean(ce_sums, dim=1)\n",
    "    ce_scores.append(ce)\n",
    "ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging the cross entropy losses of each sequence  within batch 1:\n",
    "Note, in practice this step will be done over each mini-batch by keeping a running average of the losses for each batch. It essentially sums up what we calculated for the cross entropy (loss for each sequence in batch 1) and divides it by the number of sequences in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2602, grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How did we do?\n",
    "A batch loss of 1.2602 is high and means that we have plenty room for improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(yhat, y):\n",
    "    return -torch.mean(torch.sum(y * torch.log(yhat), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(predictions, y_true):\n",
    "    total_loss = 0.0\n",
    "    for prediction, label in zip(predictions, y_true):\n",
    "        cross = cross_entropy(prediction, label)\n",
    "        total_loss += cross\n",
    "    return total_loss/ len(predictions)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [Wz, Wr, Wh, Uh, Uz, Ur, bz, br, bh, Wy, by] # iterable of parameters that require gradient computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100\n",
      "Loss: 0.06194562092423439\n",
      "Matttttttttt\n",
      "Epoch: 11/100\n",
      "Loss: 0.06142784655094147\n",
      "Matttttttttt\n",
      "Epoch: 21/100\n",
      "Loss: 0.06091505289077759\n",
      "MatttttttttM\n",
      "Epoch: 31/100\n",
      "Loss: 0.060407400131225586\n",
      "Matttttttttt\n",
      "Epoch: 41/100\n",
      "Loss: 0.05990474298596382\n",
      "Matttttttttt\n",
      "Epoch: 51/100\n",
      "Loss: 0.05940718576312065\n",
      "Matttttttttt\n",
      "Epoch: 61/100\n",
      "Loss: 0.058914750814437866\n",
      "Matttttttttt\n",
      "Epoch: 71/100\n",
      "Loss: 0.05842741206288338\n",
      "Matttttttttt\n",
      "Epoch: 81/100\n",
      "Loss: 0.05794508382678032\n",
      "Matttttttttt\n",
      "Epoch: 91/100\n",
      "Loss: 0.057467926293611526\n",
      "Matttttttttt\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params, lr = 0.01)\n",
    "max_epochs = 100  # passes through the data\n",
    "for e in range(max_epochs):\n",
    "    h = torch.zeros(batch_size, hiddenSize)\n",
    "    for i in range(num_batches):\n",
    "        x_in = X[i]\n",
    "        y_in = y[i]\n",
    "        \n",
    "        optimizer.zero_grad() # zero out gradients \n",
    "        \n",
    "        out, h = gru(x_in, h)\n",
    "        loss = total_loss(out, y_in)\n",
    "        loss.backward(retain_graph=True) # backpropagate through time to adjust the weights and find the gradients of the loss function\n",
    "        optimizer.step()\n",
    "    if e % 10 == 0:\n",
    "        print(f'Epoch: {e+1}/{max_epochs}')\n",
    "        print(f'Loss: {loss}')\n",
    "        print(sample('Ma',10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "So we optimized reduced our loss and we are not predicting well...why? Well, as mentioned in the first tutorial this is an extremely small dataset, when training on a neural net made from scratch. It is recommended that you do so with lots of data. However, the purpose of this tutorial is not to create the high performance neural net, but to demonstrate what goes on under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation:\n",
    "The final step involves a backward pass through the algorithm. This step is called backpropagation, and it involves understanding the impact of adjusting the weights on the cost function. This is done by calculating the error vectors $\\delta$ starting from the final layer backward by repeatedly applying the chain rule through each layer. For more detailed proof of back-prop through time: https://github.com/tianyic/LSTM-GRU/blob/master/MTwrtieup.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "1. The Unreasonable Effectiveness of Recurrent Neural Networks\n",
    "2. Udacity Deep Learning with Pytorch\n",
    "3. Fastai Deep Learning for Coders\n",
    "4. Deep Learning - The Straight Dope (RNNs)\n",
    "5. Deep Learning Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
